{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.72s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "\n",
    "data_dir = '../DATA/coco'\n",
    "annotation_file = os.path.join(data_dir, 'annotations/instances_train2017.json')\n",
    "\n",
    "# COCOデータセットを読み込む\n",
    "coco = COCO(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def reshape_func(image_file, target):#sslに入れるための正方形の画像とそれに対応する計算しなおしたbboxを出力する\n",
    "\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406] #imagenetの正規化\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "    IMAGENET_SIZE = 224\n",
    "\n",
    "    #２枚の画像を比べる(余白なし)\n",
    "\n",
    "    transform1 = transforms.Resize(224)\n",
    "    transform2 = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((IMAGENET_MEAN), (IMAGENET_STD))\n",
    "        ])\n",
    "\n",
    "    # 画像ファイルのパスを指定する\n",
    "\n",
    "    img = cv2.imread(image_file)#PILだとshapeがよくわからんからcv2で読み込み\n",
    "    reshaeped_im = transform1(Image.open(image_file))\n",
    "    transformed_image = transform2(reshaeped_im).to(\"cuda\")#cv2からだとtorchに入んないからPILで読み込み(いらなそう)\n",
    "\n",
    "    new_width = reshaeped_im.size[0]\n",
    "    new_height = reshaeped_im.size[1]\n",
    "\n",
    "\n",
    "    resize_ratio_x = new_width / img.shape[1]\n",
    "    resize_ratio_y = new_height / img.shape[0]\n",
    "\n",
    "\n",
    "    # 画像ファイル名から画像IDを取得する\n",
    "    image_id = None\n",
    "    for image_info in coco.dataset['images']:\n",
    "        if image_info['file_name'] == os.path.basename(image_file):\n",
    "            image_id = image_info['id']\n",
    "            break\n",
    "\n",
    "    if image_id is not None:\n",
    "        # 画像IDに対応するアノテーション情報を取得する\n",
    "        annotations_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(annotations_ids)\n",
    "\n",
    "        # BBOXとラベルを表示する\n",
    "        for annotation in annotations:\n",
    "            bbox = annotation['bbox']\n",
    "            resized_bbox = [\n",
    "            int((bbox[0] * resize_ratio_x) - ((reshaeped_im.size[0] - 224) / 2)),\n",
    "            int((bbox[1] * resize_ratio_y) - ((reshaeped_im.size[1] - 224) / 2)),\n",
    "            int(bbox[2] * resize_ratio_x),\n",
    "            int(bbox[3] * resize_ratio_y)\n",
    "            ]\n",
    "            label = coco.loadCats(annotation['category_id'])[0]['name']\n",
    "            if label == target:\n",
    "                \n",
    "                return transformed_image, resized_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishido/.pyenv/versions/anaconda3-2023.03/envs/coco/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yishido/.pyenv/versions/anaconda3-2023.03/envs/coco/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=14, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(14, 14), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=14, shift_size=7 mlp_ratio=4\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(14, 14), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=14, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(14, 14), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=14, shift_size=7 mlp_ratio=4\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(14, 14), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(14, 14), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=14, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(14, 14), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.swin_transformer import SwinTransformer\n",
    "\n",
    "model = SwinTransformer(\n",
    "    in_chans=3,\n",
    "    patch_size=4,\n",
    "    embed_dim=96,\n",
    "    depths=[2,2,6,2],\n",
    "    num_heads=[3,6,12,24],\n",
    "    window_size=14,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0,\n",
    "    attn_drop_rate=0,\n",
    "    drop_path_rate= 0,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    ")\n",
    "\n",
    "state_dict = torch.load(\"/home/yishido/pretrained_model/esvit/checkpoint_best.pth\", map_location=\"cpu\")\n",
    "state_dict = state_dict[\"teacher\"]\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)#mlpheadはいらないよね？\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(target, inputs):#抽出する関数\n",
    "    feature = None\n",
    "\n",
    "    def forward_hook(module, inputs, outputs):\n",
    "        # 順伝搬の出力を features というグローバル変数に記録する\n",
    "        global features\n",
    "        # 1. detach でグラフから切り離す。\n",
    "        # 2. clone() でテンソルを複製する。モデルのレイヤーで ReLU(inplace=True) のように\n",
    "        #    inplace で行う層があると、値がその後のレイヤーで書き換えられてまい、\n",
    "        #    指定した層の出力が取得できない可能性があるため、clone() が必要。\n",
    "        features = outputs.detach().clone()\n",
    "\n",
    "    # コールバック関数を登録する。\n",
    "    handle = target.register_forward_hook(forward_hook)\n",
    "\n",
    "    # 推論する\n",
    "    model.eval()\n",
    "    model(inputs)\n",
    "\n",
    "    # コールバック関数を解除する。\n",
    "    handle.remove()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(bbox):#idをとってくるように変更済み。ちゃんととってきているか確認したい場合はcenter_pointsを返すようにする\n",
    "    # 画像のサイズとグリッドの設定\n",
    "    image_size = (224, 224)\n",
    "    grid_size = (7, 7)###################################################################################\n",
    "\n",
    "    # BBOXの座標\n",
    "\n",
    "    # グリッドのセルの幅と高さを計算\n",
    "    cell_width = image_size[0] // grid_size[0]\n",
    "    cell_height = image_size[1] // grid_size[1]\n",
    "\n",
    "    # BBOXの範囲内にあるセルの中心点を取得\n",
    "    center_points = []\n",
    "    id = []\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            center_x = i * cell_width + cell_width // 2\n",
    "            center_y = j * cell_height + cell_height // 2\n",
    "            if bbox[0] <= center_x <= bbox[0] + bbox[2] and bbox[1] <= center_y <= bbox[1] + bbox[3]:\n",
    "                center_points.append((center_x, center_y))\n",
    "                id.append([i,j])\n",
    "    \n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sim(v1, v2):#内積の関数\n",
    "    return torch.dot(v1, v2) / (torch.linalg.norm(v1) * torch.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_count(id_1,emb_1,id_2,emb_2):\n",
    "    \n",
    "    A = 0 #最大値がbboxの中に履いている個数\n",
    "\n",
    "    for i in range(len(id_1)):\n",
    "        n = id_1[i][0] #行\n",
    "        m = id_1[i][1] #列\n",
    "\n",
    "        id_emb = n*7+m \n",
    "        emb1 = emb_1[id_emb]\n",
    "        \n",
    "        inner_product = []\n",
    "        for j in range(len(emb_2)):\n",
    "            inner_product.append(dot_sim(emb1,emb_2[j]).item())\n",
    "        \n",
    "        inner_product = torch.tensor(inner_product)\n",
    "        \n",
    "        max_index = torch.argmax(inner_product).item()\n",
    "\n",
    "        k = max_index // 7 #最大値に当たるpatchの行\n",
    "        l = max_index % 7 #最大値に当たるpatchの列\n",
    "\n",
    "        max_id = [k, l] #maxのid（行，列）\n",
    "\n",
    "        if max_id in id_2:\n",
    "            A += 1\n",
    "    \n",
    "    if A == 0:\n",
    "        wariai = 0\n",
    "\n",
    "    wariai = A/len(id_1)\n",
    "\n",
    "    return wariai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "with open('cat.txt', 'r') as file:\n",
    "    B_C_path = file.read().splitlines()\n",
    "        \n",
    "with open('cat_dog.txt', 'r') as file:\n",
    "    BnC_path = file.read().splitlines()\n",
    "\n",
    "with open('dog.txt', 'r') as file:\n",
    "    C_B_path = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30111111111111105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#猫ー猫！＿犬\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(B_C_path))):\n",
    "    element1 = B_C_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../DATA/coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../DATA/coco/images/train2017/{element2}\"\n",
    "\n",
    "        # try:\n",
    "        im_1 = reshape_func(image_path1, target1) #chanelが１の場合\n",
    "        im_2 = reshape_func(image_path2, target1)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0]#[1:,]############ViTではないからclsトークンがないのかな？\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0]#[1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        # try:\n",
    "        A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        # except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "            # continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30111111111111105\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32923809523809533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#猫ー猫＿犬!\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(B_C_path))):\n",
    "    element1 = B_C_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../DATA/coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../DATA/coco/images/train2017/{element2}\"\n",
    "\n",
    "        # try:\n",
    "        im_1 = reshape_func(image_path1, target1) #chanelが１の場合\n",
    "        im_2 = reshape_func(image_path2, target2)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0]#[1:,]############ViTではないからclsトークンがないのかな？\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0]#[1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        # try:\n",
    "        A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        # except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "        #     continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32923809523809533\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13440714285714284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#犬ー猫！＿犬\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(C_B_path))):\n",
    "    element1 = C_B_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../DATA/coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../DATA/coco/images/train2017/{element2}\"\n",
    "\n",
    "        # try:\n",
    "        im_1 = reshape_func(image_path1, target2) #chanelが１の場合\n",
    "        im_2 = reshape_func(image_path2, target1)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0]#[1:,]############ViTではないからclsトークンがないのかな？\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0]#[1:,]\n",
    "        # print(len(emb_1))\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        # try:\n",
    "        A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        # except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "        #     continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13440714285714284\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35998758503401374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#犬ー猫＿犬!\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(C_B_path))):\n",
    "    element1 = C_B_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../DATA/coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../DATA/coco/images/train2017/{element2}\"\n",
    "\n",
    "        # try:\n",
    "        im_1 = reshape_func(image_path1, target2) #chanelが１の場合\n",
    "        im_2 = reshape_func(image_path2, target2)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0]#[1:,]############ViTではないからclsトークンがないのかな？\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0]#[1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        # try:\n",
    "        A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        # except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "        #     continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35998758503401374\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45302380952380955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#同じ物体同士\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(B_C_path))):\n",
    "    element1 = B_C_path[i]\n",
    "    for j in range(len(B_C_path)):\n",
    "        element2 = B_C_path[j]\n",
    "\n",
    "        image_path1 = f\"../DATA/coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../DATA/coco/images/train2017/{element2}\"\n",
    "\n",
    "        # try:\n",
    "        im_1 = reshape_func(image_path1, target1) #chanelが１の場合\n",
    "        im_2 = reshape_func(image_path2, target1)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0]#[1:,]############ViTではないからclsトークンがないのかな？\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0]#[1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        # try:\n",
    "        A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        # except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "        #     continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45302380952380955\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6490338435374151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#同じ物体同士\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(C_B_path))):\n",
    "    element1 = C_B_path[i]\n",
    "    for j in range(len(C_B_path)):\n",
    "        element2 = C_B_path[j]\n",
    "\n",
    "        image_path1 = f\"../DATA/coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../DATA/coco/images/train2017/{element2}\"\n",
    "\n",
    "        # try:\n",
    "        im_1 = reshape_func(image_path1, target2) #chanelが１の場合\n",
    "        im_2 = reshape_func(image_path2, target2)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0]#[1:,]############ViTではないからclsトークンがないのかな？\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0]#[1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        # try:\n",
    "        A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        # except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "        #     continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6490338435374151\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
