{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.swin_transformer import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinTransformer(\n",
    "    in_chans=3,\n",
    "    patch_size=4,\n",
    "    embed_dim=96,\n",
    "    depths=[2,2,6,2],\n",
    "    num_heads=[3,6,12,24],\n",
    "    window_size=14,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0,\n",
    "    attn_drop_rate=0,\n",
    "    drop_path_rate= 0,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/home/yishido/pretrained_model/esvit/checkpoint_best.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state_dict[\"teacher\"]\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head_dense.mlp.0.weight', 'head_dense.mlp.0.bias', 'head_dense.mlp.2.weight', 'head_dense.mlp.2.bias', 'head_dense.mlp.4.weight', 'head_dense.mlp.4.bias', 'head_dense.last_layer.weight_g', 'head_dense.last_layer.weight_v', 'head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict, strict=False)#mlpheadはいらないよね？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=14, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(14, 14), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=14, shift_size=7 mlp_ratio=4\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(14, 14), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=14, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(14, 14), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=14, shift_size=7 mlp_ratio=4\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(14, 14), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(14, 14), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=14, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(14, 14), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Mult-Adds\n",
       "======================================================================================================================================================\n",
       "SwinTransformer                                    [1, 3, 224, 224]          [1, 1000]                 --                        --\n",
       "├─PatchEmbed: 1-1                                  [1, 3, 224, 224]          [1, 3136, 96]             --                        --\n",
       "│    └─Conv2d: 2-1                                 [1, 3, 224, 224]          [1, 96, 56, 56]           4,704                     14,751,744\n",
       "│    └─LayerNorm: 2-2                              [1, 3136, 96]             [1, 3136, 96]             192                       192\n",
       "├─Dropout: 1-2                                     [1, 3136, 96]             [1, 3136, 96]             --                        --\n",
       "├─ModuleList: 1-3                                  --                        --                        --                        --\n",
       "│    └─BasicLayer: 2-3                             [1, 3136, 96]             [1, 784, 192]             --                        --\n",
       "│    │    └─ModuleList: 3-1                        --                        --                        228,054                   --\n",
       "│    │    └─PatchMerging: 3-2                      [1, 3136, 96]             [1, 784, 192]             74,496                    74,496\n",
       "│    └─BasicLayer: 2-4                             [1, 784, 192]             [1, 196, 384]             --                        --\n",
       "│    │    └─ModuleList: 3-3                        --                        --                        898,476                   --\n",
       "│    │    └─PatchMerging: 3-4                      [1, 784, 192]             [1, 196, 384]             296,448                   296,448\n",
       "│    └─BasicLayer: 2-5                             [1, 196, 384]             [1, 49, 768]              --                        --\n",
       "│    │    └─ModuleList: 3-5                        --                        --                        10,699,272                --\n",
       "│    │    └─PatchMerging: 3-6                      [1, 196, 384]             [1, 49, 768]              1,182,720                 1,182,720\n",
       "│    └─BasicLayer: 2-6                             [1, 49, 768]              [1, 49, 768]              --                        --\n",
       "│    │    └─ModuleList: 3-7                        --                        --                        14,183,856                --\n",
       "├─LayerNorm: 1-4                                   [1, 49, 768]              [1, 49, 768]              1,536                     1,536\n",
       "├─AdaptiveAvgPool1d: 1-5                           [1, 768, 49]              [1, 768, 1]               --                        --\n",
       "├─Linear: 1-6                                      [1, 768]                  [1, 1000]                 769,000                   769,000\n",
       "======================================================================================================================================================\n",
       "Total params: 28,338,754\n",
       "Trainable params: 28,338,754\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 45.02\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 137.29\n",
       "Params size (MB): 113.06\n",
       "Estimated Total Size (MB): 250.95\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "col_names=[\"input_size\",\"output_size\",\"num_params\", \"mult_adds\"]\n",
    "torchinfo.summary(model, col_names=col_names, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(target, inputs):#抽出する関数\n",
    "    feature = None\n",
    "\n",
    "    def forward_hook(module, inputs, outputs):\n",
    "        # 順伝搬の出力を features というグローバル変数に記録する\n",
    "        global features\n",
    "        # 1. detach でグラフから切り離す。\n",
    "        # 2. clone() でテンソルを複製する。モデルのレイヤーで ReLU(inplace=True) のように\n",
    "        #    inplace で行う層があると、値がその後のレイヤーで書き換えられてまい、\n",
    "        #    指定した層の出力が取得できない可能性があるため、clone() が必要。\n",
    "        features = outputs.detach().clone()\n",
    "\n",
    "    # コールバック関数を登録する。\n",
    "    handle = target.register_forward_hook(forward_hook)\n",
    "\n",
    "    # 推論する\n",
    "    model.eval()\n",
    "    model(inputs)\n",
    "\n",
    "    # コールバック関数を解除する。\n",
    "    handle.remove()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "image = torch.randn(1,3,224,224).to(\"cuda\")\n",
    "target_module = model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = extract(target_module, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
