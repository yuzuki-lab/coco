{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/home/yishido/dino/save_global_crop/checkpoint0040.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vision_transformer as vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vits.__dict__['vit_base'](patch_size=16, num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = state_dict[\"teacher\"]\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        # remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.19s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "\n",
    "data_dir = '../coco'\n",
    "annotation_file = os.path.join(data_dir, 'annotations/instances_train2017.json')\n",
    "\n",
    "# COCOデータセットを読み込む\n",
    "coco = COCO(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def reshape_func(image_file, target):#sslに入れるための正方形の画像とそれに対応する計算しなおしたbboxを出力する\n",
    "\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406] #imagenetの正規化\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "    IMAGENET_SIZE = 224\n",
    "\n",
    "    #２枚の画像を比べる(余白なし)\n",
    "\n",
    "    transform1 = transforms.Resize(224)\n",
    "    transform2 = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((IMAGENET_MEAN), (IMAGENET_STD))\n",
    "        ])\n",
    "\n",
    "    # 画像ファイルのパスを指定する\n",
    "\n",
    "    img = cv2.imread(image_file)#PILだとshapeがよくわからんからcv2で読み込み\n",
    "    reshaeped_im = transform1(Image.open(image_file))\n",
    "    transformed_image = transform2(reshaeped_im).to(\"cuda\")#cv2からだとtorchに入んないからPILで読み込み(いらなそう)\n",
    "\n",
    "    new_width = reshaeped_im.size[0]\n",
    "    new_height = reshaeped_im.size[1]\n",
    "\n",
    "\n",
    "    resize_ratio_x = new_width / img.shape[1]\n",
    "    resize_ratio_y = new_height / img.shape[0]\n",
    "\n",
    "\n",
    "    # 画像ファイル名から画像IDを取得する\n",
    "    image_id = None\n",
    "    for image_info in coco.dataset['images']:\n",
    "        if image_info['file_name'] == os.path.basename(image_file):\n",
    "            image_id = image_info['id']\n",
    "            break\n",
    "\n",
    "    if image_id is not None:\n",
    "        # 画像IDに対応するアノテーション情報を取得する\n",
    "        annotations_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(annotations_ids)\n",
    "\n",
    "        # BBOXとラベルを表示する\n",
    "        for annotation in annotations:\n",
    "            bbox = annotation['bbox']\n",
    "            resized_bbox = [\n",
    "            int((bbox[0] * resize_ratio_x) - ((reshaeped_im.size[0] - 224) / 2)),\n",
    "            int((bbox[1] * resize_ratio_y) - ((reshaeped_im.size[1] - 224) / 2)),\n",
    "            int(bbox[2] * resize_ratio_x),\n",
    "            int(bbox[3] * resize_ratio_y)\n",
    "            ]\n",
    "            label = coco.loadCats(annotation['category_id'])[0]['name']\n",
    "            if label == target:\n",
    "                \n",
    "                return transformed_image, resized_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitb16 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')##############################################################################\n",
    "# model = vitb16.to(\"cuda\")\n",
    "\n",
    "def extract(target, inputs):#抽出する関数\n",
    "    feature = None\n",
    "\n",
    "    def forward_hook(module, inputs, outputs):\n",
    "        # 順伝搬の出力を features というグローバル変数に記録する\n",
    "        global features\n",
    "        # 1. detach でグラフから切り離す。\n",
    "        # 2. clone() でテンソルを複製する。モデルのレイヤーで ReLU(inplace=True) のように\n",
    "        #    inplace で行う層があると、値がその後のレイヤーで書き換えられてまい、\n",
    "        #    指定した層の出力が取得できない可能性があるため、clone() が必要。\n",
    "        features = outputs.detach().clone()\n",
    "\n",
    "    # コールバック関数を登録する。\n",
    "    handle = target.register_forward_hook(forward_hook)\n",
    "\n",
    "    # 推論する\n",
    "    model.eval()\n",
    "    model(inputs)\n",
    "\n",
    "    # コールバック関数を解除する。\n",
    "    handle.remove()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(bbox):#idをとってくるように変更済み。ちゃんととってきているか確認したい場合はcenter_pointsを返すようにする\n",
    "    # 画像のサイズとグリッドの設定\n",
    "    image_size = (224, 224)\n",
    "    grid_size = (14, 14)###################################################################################\n",
    "\n",
    "    # BBOXの座標\n",
    "\n",
    "    # グリッドのセルの幅と高さを計算\n",
    "    cell_width = image_size[0] // grid_size[0]\n",
    "    cell_height = image_size[1] // grid_size[1]\n",
    "\n",
    "    # BBOXの範囲内にあるセルの中心点を取得\n",
    "    center_points = []\n",
    "    id = []\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            center_x = i * cell_width + cell_width // 2\n",
    "            center_y = j * cell_height + cell_height // 2\n",
    "            if bbox[0] <= center_x <= bbox[0] + bbox[2] and bbox[1] <= center_y <= bbox[1] + bbox[3]:\n",
    "                center_points.append((center_x, center_y))\n",
    "                id.append([i,j])\n",
    "    \n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_count(id_1,emb_1,id_2,emb_2):\n",
    "    \n",
    "    A = 0 #最大値がbboxの中に履いている個数\n",
    "\n",
    "    for i in range(len(id_1)):\n",
    "        n = id_1[i][0] #行\n",
    "        m = id_1[i][1] #列\n",
    "\n",
    "        id_emb = n*14+m #196の中のどこに当たるのか###########################################\n",
    "        # print(id_emb)\n",
    "        emb1 = emb_1[id_emb]\n",
    "        # print(emb1.shape)\n",
    "\n",
    "        inner_product = torch.matmul(emb_2,emb1)\n",
    "        max_index = torch.argmax(inner_product).item()\n",
    "\n",
    "        k = max_index // 14 #最大値に当たるpatchの行##################################################\n",
    "        l = max_index % 14 #最大値に当たるpatchの列\n",
    "\n",
    "        max_id = [k, l] #maxのid（行，列）\n",
    "        # print(f\"maxid:{max_id}\")\n",
    "\n",
    "        if max_id in id_2:\n",
    "            A += 1\n",
    "    \n",
    "    # print(f\"count数:{A}\")\n",
    "    # print(f\"bbox内のpatchの数:{len(id_1)}\")\n",
    "\n",
    "    if A == 0:\n",
    "        wariai = 0\n",
    "\n",
    "    wariai = A/len(id_1)\n",
    "\n",
    "    return wariai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "with open('B_C.txt', 'r') as file:\n",
    "    B_C_path = file.read().splitlines()\n",
    "        \n",
    "with open('BnC.txt', 'r') as file:\n",
    "    BnC_path = file.read().splitlines()\n",
    "\n",
    "with open('C_B.txt', 'r') as file:\n",
    "    C_B_path = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [43:57<00:00, 26.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21017548932350577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#猫ー猫！＿犬\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(B_C_path))):\n",
    "    element1 = B_C_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../coco/images/train2017/{element2}\"\n",
    "\n",
    "        try:\n",
    "            im_1 = reshape_func(image_path1, target1) #chanelが１の場合\n",
    "            im_2 = reshape_func(image_path2, target1)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0][1:,]\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0][1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        try:\n",
    "            A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "            continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21017548932350577\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [43:22<00:00, 26.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3088704820011293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#猫ー猫＿犬!\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(B_C_path))):\n",
    "    element1 = B_C_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../coco/images/train2017/{element2}\"\n",
    "\n",
    "        try:\n",
    "            im_1 = reshape_func(image_path1, target1) #chanelが１の場合\n",
    "            im_2 = reshape_func(image_path2, target2)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0][1:,]\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0][1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        try:\n",
    "            A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "            continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3088704820011293\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [45:52<00:00, 27.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20215507962746532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#犬ー猫！＿犬\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(C_B_path))):\n",
    "    element1 = C_B_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../coco/images/train2017/{element2}\"\n",
    "\n",
    "        try:\n",
    "            im_1 = reshape_func(image_path1, target2) #chanelが１の場合\n",
    "            im_2 = reshape_func(image_path2, target1)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0][1:,]\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0][1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        try:\n",
    "            A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "            continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20215507962746532\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [42:25<02:39, 26.59s/it]"
     ]
    }
   ],
   "source": [
    "#犬ー猫＿犬!\n",
    "\n",
    "target1 = 'cat'\n",
    "target2 = 'dog'\n",
    "\n",
    "allcount=[]\n",
    "\n",
    "for i in tqdm(range(len(C_B_path))):\n",
    "    element1 = C_B_path[i]\n",
    "    for j in range(len(BnC_path)):\n",
    "        element2 = BnC_path[j]\n",
    "\n",
    "        image_path1 = f\"../coco/images/train2017/{element1}\"\n",
    "        image_path2 = f\"../coco/images/train2017/{element2}\"\n",
    "\n",
    "        try:\n",
    "            im_1 = reshape_func(image_path1, target2) #chanelが１の場合\n",
    "            im_2 = reshape_func(image_path2, target2)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        target_module = model.norm\n",
    "        emb_1 = extract(target_module, im_1[0].unsqueeze(0))[0][1:,]\n",
    "        emb_2 = extract(target_module, im_2[0].unsqueeze(0))[0][1:,]\n",
    "\n",
    "        id_in_bbox_1 = get_id(im_1[1])\n",
    "        id_in_bbox_2 = get_id(im_2[1])\n",
    "        # print(dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2))\n",
    "\n",
    "        try:\n",
    "            A = dot_count(id_in_bbox_1, emb_1, id_in_bbox_2, emb_2) #\n",
    "\n",
    "        except Exception as e: #元になるbboxが小さすぎたせいで、idが取れなかった場合\n",
    "            continue\n",
    "\n",
    "        # print(A)\n",
    "        allcount.append(A)\n",
    "\n",
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3006360174064968\n"
     ]
    }
   ],
   "source": [
    "print(sum(allcount)/len(allcount))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todolist\n",
    "1.4パターン試す<br>\n",
    "2.その間codeがあっているかの確認<br>\n",
    "3.各モデルで実行する<br>\n",
    "4.論文かく！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
